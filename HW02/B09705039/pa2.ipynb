{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7f35a4-d641-4aea-8200-798f18bab7bc",
   "metadata": {},
   "source": [
    "## B09705039_HW02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b69902-53b6-44d2-84c2-249e65d5855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53704ab0-95cf-4abb-a627-4ec1da6657dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tokenized doc result\n",
    "doc_amount = 1095\n",
    "all_doc = []\n",
    "for doc in range(1, doc_amount + 1):\n",
    "    # Read file.\n",
    "    path = \"./data/\" + str(doc) + \".txt\"\n",
    "    f = open(path, 'r')\n",
    "    all_text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Tokenization.\n",
    "    # signs that can be ignored: We only listed a few here, if needed more we can add more.\n",
    "    nonAlphanumeric = [\",\", \"'\", \";\", \":\", '\"', \"@\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"<\", \">\", \"=\", \"+\", \"^\", \"$\", \"~\", \"*\", \"/\", \"{\", \"}\", \"&\", \"#\", \"%\",\"`\", \"_\"]\n",
    "    for i in nonAlphanumeric:\n",
    "        all_text = all_text.replace(i, \" \")\n",
    "\n",
    "    # periods: concatenate\n",
    "    all_text = all_text.replace(\".\", \"\")\n",
    "    # hyphens: concatenate\n",
    "    all_text = all_text.replace(\"-\", \"\")\n",
    "    \n",
    "    # remove digits\n",
    "    all_text = ''.join([i for i in all_text if not i.isdigit()])\n",
    "\n",
    "    tokenize = all_text.split()\n",
    "\n",
    "    # Lowercasing everything.\n",
    "    lowercase = []\n",
    "    for i in tokenize:\n",
    "        lowercase.append(i.lower())\n",
    "        \n",
    "    # Stopword removal.\n",
    "    # Read stopwords file. stopwords.txt is generated by nltk.\n",
    "    path = 'stopwords.txt'\n",
    "    f2 = open(path, 'r')\n",
    "    stop_words = f2.read()\n",
    "    f2.close()\n",
    "    \n",
    "    # Removal start.\n",
    "    stop_words = stop_words.split()\n",
    "    stopword_removed = []\n",
    "    for w in lowercase:\n",
    "        if w not in stop_words:\n",
    "            stopword_removed.append(w)\n",
    "        \n",
    "    # Stemming using Porterâ€™s algorithm.\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    after_stemming = []\n",
    "    for w in stopword_removed:\n",
    "        after_stemming.append(ps.stem(w))\n",
    "        \n",
    "    all_doc.append(after_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54b5de4-480c-422e-b5bb-5359d7350615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf, df dict\n",
    "tf_dict_arr = []\n",
    "df_dict = {}\n",
    "for i in range(doc_amount):\n",
    "    # count tf\n",
    "    temp_tf_dict = {}\n",
    "    for j in all_doc[i]:\n",
    "        if j not in temp_tf_dict.keys():\n",
    "            temp_tf_dict[j] = 1\n",
    "        else:\n",
    "            temp_tf_dict[j] += 1\n",
    "    tf_dict_arr.append(temp_tf_dict)\n",
    "    # count df\n",
    "    for x in temp_tf_dict.keys():\n",
    "        if x not in df_dict.keys():\n",
    "            df_dict[x] = 1\n",
    "        else:\n",
    "            df_dict[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc1464b-7460-4f10-a345-75a08eefb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dictionary.txt\n",
    "path = 'dictionary.txt'\n",
    "f = open(path, 'w')\n",
    "\n",
    "sortednames = sorted(df_dict.keys(), key=lambda x:x.lower())\n",
    "writeArr = []\n",
    "count = 1\n",
    "writeArr.append(\"t_index \" + \"term \" + \"df\" + \"\\n\")\n",
    "for key in sortednames:\n",
    "    writeArr.append(str(count)+ \" \")\n",
    "    writeArr.append(key + \" \")\n",
    "    writeArr.append(str(df_dict[key]) + \"\\n\")\n",
    "    count += 1\n",
    "f.writelines(writeArr)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8e095d-8e04-4636-b158-4a278b24b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tf-idf unit vector\n",
    "# Calculate idf\n",
    "idf = {}\n",
    "for i in sortednames:\n",
    "    idf[i] = math.log10(doc_amount/df_dict[i])\n",
    "# Calculate each document's tf-idf unit vector\n",
    "tf_idf_norm = []\n",
    "for i in tf_dict_arr:\n",
    "    tf_idf_temp = {}\n",
    "    norm = 0\n",
    "    for j in i.keys():\n",
    "        tf_idf_temp[sortednames.index(j)] = i[j] * idf[j]\n",
    "        norm += (tf_idf_temp[sortednames.index(j)]) ** 2\n",
    "    norm = math.sqrt(norm)\n",
    "    for j in i.keys():\n",
    "        tf_idf_temp[sortednames.index(j)] = tf_idf_temp[sortednames.index(j)] / norm\n",
    "    tf_idf_norm.append(tf_idf_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934c6133-b386-41b9-b08f-faf56026f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output docID.txt\n",
    "for doc in range(1, doc_amount + 1):\n",
    "    path = \"./output/doc\" + str(doc) + \".txt\"\n",
    "    f = open(path, 'w')\n",
    "\n",
    "    sortednames = sorted(tf_idf_norm[doc - 1].keys(), key=lambda x:x)\n",
    "    writeArr = []\n",
    "    writeArr.append(str(len(sortednames)) + \"\\n\")\n",
    "    writeArr.append(\"t_index \" + \"tf-idf\" + \"\\n\")\n",
    "    for key in sortednames:\n",
    "        writeArr.append(str(key + 1) + \" \")\n",
    "        writeArr.append(str(tf_idf_norm[doc - 1][key]) + \"\\n\")\n",
    "    f.writelines(writeArr)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16067ae6-8a2d-4d3e-bda2-7a2e7a0912e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between document 1 and 2 is 0.18056710715588556\n"
     ]
    }
   ],
   "source": [
    "# Count cosine similarity\n",
    "def cosine(Docx, Docy):\n",
    "    cs = 0\n",
    "    for i in Docx.keys():\n",
    "        if i in Docy.keys():\n",
    "            cs += (Docx[i] * Docy[i])\n",
    "    return cs\n",
    "\n",
    "Docx = tf_idf_norm[0]\n",
    "Docy = tf_idf_norm[1]\n",
    "cs = cosine(Docx, Docy)\n",
    "print(\"The cosine similarity between document 1 and 2 is\", cs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
