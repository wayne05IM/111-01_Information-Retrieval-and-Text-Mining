{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f62c1c0-746f-44c0-93f8-157e1ab3dcce",
   "metadata": {},
   "source": [
    "# B09705039_HW03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b20a99-e2bd-46ff-9fbe-cf09a2264c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23707970-9549-40e8-9317-29f26a5a08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,   19,   29,  113,  115,  169,  278,  301,  316,  317,  321,\n",
       "         324,  325,  338,  341],\n",
       "       [   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   12,\n",
       "          13,   14,   15,   16],\n",
       "       [ 813,  817,  818,  819,  820,  821,  822,  824,  825,  826,  828,\n",
       "         829,  830,  832,  833],\n",
       "       [ 635,  680,  683,  702,  704,  705,  706,  708,  709,  719,  720,\n",
       "         722,  723,  724,  726],\n",
       "       [ 646,  751,  781,  794,  798,  799,  801,  812,  815,  823,  831,\n",
       "         839,  840,  841,  842],\n",
       "       [ 995,  998,  999, 1003, 1005, 1006, 1007, 1009, 1011, 1012, 1013,\n",
       "        1014, 1015, 1016, 1019],\n",
       "       [ 700,  730,  731,  732,  733,  735,  740,  744,  752,  754,  755,\n",
       "         756,  757,  759,  760],\n",
       "       [ 262,  296,  304,  308,  337,  397,  401,  443,  445,  450,  466,\n",
       "         480,  513,  533,  534],\n",
       "       [ 130,  131,  132,  133,  134,  135,  136,  137,  138,  139,  140,\n",
       "         141,  142,  143,  145],\n",
       "       [  31,   44,   70,   83,   86,   92,  100,  102,  305,  309,  315,\n",
       "         320,  326,  327,  328],\n",
       "       [ 240,  241,  243,  244,  245,  248,  250,  254,  255,  256,  258,\n",
       "         260,  275,  279,  295],\n",
       "       [ 535,  542,  571,  573,  574,  575,  576,  578,  581,  582,  583,\n",
       "         584,  585,  586,  588],\n",
       "       [ 485,  520,  523,  526,  527,  529,  530,  531,  532,  536,  537,\n",
       "         538,  539,  540,  541]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training file.\n",
    "path = \"./training.txt\"\n",
    "f = open(path, 'r')\n",
    "training_raw = f.read()\n",
    "f.close()\n",
    "\n",
    "# training data\n",
    "training_raw = training_raw.split(\"\\n\")\n",
    "training = []\n",
    "for i in training_raw:\n",
    "    training.append(i.split(\" \"))\n",
    "training = np.delete(training, [0, 16], 1)\n",
    "training = np.array(training).astype(int)\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a887a1-c056-42bc-bcd2-ba1caf88aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "testing = []\n",
    "for i in range(1, 1096):\n",
    "    if i not in training:\n",
    "        testing.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3ad419-5a27-48e1-8081-a62c7975a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tokenized doc result\n",
    "doc_amount = 1095\n",
    "all_doc = []\n",
    "for doc in range(1, doc_amount + 1):\n",
    "    # Read file.\n",
    "    path = \"./data/\" + str(doc) + \".txt\"\n",
    "    f = open(path, 'r')\n",
    "    all_text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Tokenization.\n",
    "    # signs that can be ignored: We only listed a few here, if needed more we can add more.\n",
    "    nonAlphanumeric = [\",\", \"'\", \";\", \":\", '\"', \"@\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"<\", \">\", \"=\", \"+\", \"^\", \"$\", \"~\", \"*\", \"/\", \"{\", \"}\", \"&\", \"#\", \"%\",\"`\", \"_\"]\n",
    "    for i in nonAlphanumeric:\n",
    "        all_text = all_text.replace(i, \" \")\n",
    "\n",
    "    # periods: concatenate\n",
    "    all_text = all_text.replace(\".\", \"\")\n",
    "    # hyphens: concatenate\n",
    "    all_text = all_text.replace(\"-\", \"\")\n",
    "    \n",
    "    # remove digits\n",
    "    all_text = ''.join([i for i in all_text if not i.isdigit()])\n",
    "\n",
    "    tokenize = all_text.split()\n",
    "\n",
    "    # Lowercasing everything.\n",
    "    lowercase = []\n",
    "    for i in tokenize:\n",
    "        lowercase.append(i.lower())\n",
    "        \n",
    "    # Stopword removal.\n",
    "    # Read stopwords file. stopwords.txt is generated by nltk.\n",
    "    path = 'stopwords.txt'\n",
    "    f2 = open(path, 'r')\n",
    "    stop_words = f2.read()\n",
    "    f2.close()\n",
    "    \n",
    "    # Removal start.\n",
    "    stop_words = stop_words.split()\n",
    "    stopword_removed = []\n",
    "    for w in lowercase:\n",
    "        if w not in stop_words:\n",
    "            stopword_removed.append(w)\n",
    "        \n",
    "    # Stemming using Porterâ€™s algorithm.\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    after_stemming = []\n",
    "    for w in stopword_removed:\n",
    "        after_stemming.append(ps.stem(w))\n",
    "        \n",
    "    all_doc.append(after_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1c6c99-2341-46ce-9a53-d03f6903e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "# ExtractVocabulary\n",
    "V = []\n",
    "for i in training:\n",
    "    for j in i:\n",
    "        for k in all_doc[j - 1]:\n",
    "            if k not in V:\n",
    "                V.append(k)\n",
    "                \n",
    "# CountDocs\n",
    "N = 0\n",
    "for c in range(13):\n",
    "    N += len(training[c])\n",
    "\n",
    "chi_dict = {}\n",
    "for t in V:\n",
    "    # get matrix\n",
    "    mat = np.zeros((13, 2))\n",
    "    for k in range(13):\n",
    "        for j in training[k]:\n",
    "            if t in all_doc[j - 1]:\n",
    "                mat[k][0] += 1\n",
    "            else:\n",
    "                mat[k][1] += 1\n",
    "                \n",
    "    # count chi-square\n",
    "    rowsum = np.sum(mat, axis=0)\n",
    "    columnsum = np.sum(mat, axis=1)\n",
    "\n",
    "    chi_sqr = 0\n",
    "    for i in range(2):\n",
    "        for j in range(13):\n",
    "            E = N * (rowsum[i] / N) * (columnsum[j] / N)\n",
    "            chi_sqr += (((mat[j][i] - E) ** 2) / E)\n",
    "            \n",
    "    chi_dict[t] = chi_sqr\n",
    "\n",
    "V = sorted(chi_dict, key=chi_dict.get, reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2913f5f5-3a22-4ace-8db4-482b8819cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "prior = []\n",
    "condprob = []\n",
    "for c in range(13):\n",
    "    # get P_c\n",
    "    N_c = len(training[c])\n",
    "    prior.append(N_c / N)\n",
    "                \n",
    "    # get tf of D in c\n",
    "    tf_dict = dict.fromkeys(V, 0)\n",
    "    for i in training[c]:\n",
    "        for j in all_doc[i - 1]:\n",
    "            if j in V:\n",
    "                tf_dict[j] += 1\n",
    "    \n",
    "    # get Pt_c\n",
    "    tnum = sum(tf_dict.values()) + len(V)\n",
    "    temp_dict = {}\n",
    "    for t in V:\n",
    "        temp_dict[t] = (tf_dict[t] + 1) / tnum\n",
    "    condprob.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fae891-8ef0-4451-8b95-f8f2d73df56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "result = []\n",
    "for test in testing:\n",
    "    score = []\n",
    "    for c in range(13):\n",
    "        temp_score = 0\n",
    "        temp_score += math.log(prior[c])\n",
    "        for t in all_doc[test - 1]:\n",
    "            if t in condprob[c]:\n",
    "                temp_score += math.log(condprob[c][t])\n",
    "        score.append(temp_score)\n",
    "    result.append(np.argmax(score) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbfe8c3-d6e6-4599-afb8-771e4f8db78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "data = {\"Id\": testing,\n",
    "       \"Value\": result}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('output.csv', index = False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
